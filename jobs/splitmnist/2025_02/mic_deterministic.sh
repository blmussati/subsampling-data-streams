#!/bin/bash

# Acquire 1 sample at time using method 'mic_deterministic' (the version of InfoRS paper). Each experiment is a different threshold (ie. number of training points to acquire)
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=10    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=20    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=50    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=100    acquisition.method=mic_deterministic

# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=10    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=20    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=50    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=100    acquisition.method=mic_deterministic

# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=10    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=20    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=50    acquisition.method=mic_deterministic
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=100    acquisition.method=mic_deterministic

# # Baseline, acquire using 'random' but keeping all other hyperparams the same
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=10    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=20    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=50    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=94    acquisition.n_train_labels_end=100    acquisition.method=random

# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=10    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=20    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=50    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=1000    acquisition.n_train_labels_end=100    acquisition.method=random

# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=10    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=20    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=50    acquisition.method=random
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.target=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=10000    acquisition.n_train_labels_end=100    acquisition.method=random

# Baseline, train on all traing data available, without acquiring points
# python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.train=-1    data.label_counts_main.target=0    data.label_counts_main.pool=0    data.label_counts_main.val=0    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=469    acquisition.n_train_labels_end=0
python    cal.py    rng.seed=0    experiment_name=splitmnist_semi    data=splitmnist/embedding_curated    data.label_counts_main.train=-1    data.label_counts_main.target=0    data.label_counts_main.pool=0    data.label_counts_main.val.0.n_per_class=30    data.batch_sizes.train=128    model=pytorch_fc_net_2layer    trainer=pytorch_neural_net_classif    trainer.optimizer.lr=0.03    trainer.optimizer.weight_decay=0.0    trainer.n_optim_steps_max=469    acquisition.n_train_labels_end=0
